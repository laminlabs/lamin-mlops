{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad6638d",
   "metadata": {},
   "source": [
    "[![](https://img.shields.io/badge/Source%20on%20GitHub-orange)](https://github.com/laminlabs/lamin-mlops/blob/main/docs/mlflow.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a564bb2",
   "metadata": {},
   "source": [
    "# MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f834cd3",
   "metadata": {},
   "source": [
    "We show how LaminDB can be integrated with [MLflow](https://mlflow.org/) to track the training process and associate datasets & parameters with models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d25798",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# !pip install 'lamindb[jupyter]' torchvision lightning wandb\n",
    "!lamin init --storage ./lamin-mlops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75fb35e",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import lamindb as ln\n",
    "import mlflow\n",
    "import lightning\n",
    "\n",
    "from torch import utils\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from autoencoder import LitAutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7acdd3",
   "metadata": {},
   "source": [
    "```{dropdown} Tracking models in both LaminDB and MLFlow\n",
    "````{note}\n",
    "It is not always necessary to track all model parameters and metrics in both LaminDB and MLFlow.\n",
    "However, if specific artifacts or runs should be queryable by specific model attributes such as, for example, the learning rate, then these attributes should be tracked.\n",
    "Below, we show exemplary how to do that for the batch size and learning rate but the approach generalizes to more features.\n",
    "````\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model run parameters & features\n",
    "MODEL_CONFIG = {\"batch_size\": 32, \"lr\": 0.001}\n",
    "\n",
    "# TODO redo this with Feature.from_dict\n",
    "hyperparameter = ln.Feature(name=\"Autoencoder hyperparameter\", is_type=True).save()\n",
    "for param_name, param_value in MODEL_CONFIG.items():\n",
    "    ln.Feature(\n",
    "        name=param_name, dtype=type(param_value).__name__, type=hyperparameter\n",
    "    ).save()\n",
    "\n",
    "ln.track(params=MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd5cb6",
   "metadata": {},
   "source": [
    "## Define a model\n",
    "\n",
    "We use a basic PyTorch Lightning autoencoder as an example model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf6162",
   "metadata": {},
   "source": [
    "````{dropdown} Code of LitAutoEncoder\n",
    "```{eval-rst}\n",
    ".. literalinclude:: autoencoder.py\n",
    "   :language: python\n",
    "   :caption: Simple autoencoder model\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0c69c",
   "metadata": {},
   "source": [
    "## Query & download the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82194a8",
   "metadata": {},
   "source": [
    "We saved the MNIST dataset in [curation notebook](/mnist) which now shows up in the Artifact registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892070e",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.Artifact.filter(kind=\"dataset\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3793e5",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "source": [
    "On LaminHub it looks like this:\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/LlMSvBjHuXbs36TBGoCM.png\" alt=\"instance view\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675b92c",
   "metadata": {},
   "source": [
    "Let's get the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5ed42",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "artifact = ln.Artifact.get(key=\"testdata/mnist\")\n",
    "artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0ce2d",
   "metadata": {},
   "source": [
    "And download it to a local cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41faeabe",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "path = artifact.cache()\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a8d60",
   "metadata": {},
   "source": [
    "Create a PyTorch-compatible dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0771f901",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = MNIST(path.as_posix(), transform=ToTensor())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c377f4e9",
   "metadata": {},
   "source": [
    "## Monitor training with MLflow\n",
    "\n",
    "Train our example model and track the training progress with `MLflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43606e58",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# enable MLFlow PyTorch autologging\n",
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf383091",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run() as mlflow_run:\n",
    "    train_dataset = MNIST(\n",
    "        root=\"./data\", train=True, download=True, transform=ToTensor()\n",
    "    )\n",
    "    train_loader = utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "    # Initialize model\n",
    "    autoencoder = LitAutoEncoder(32, 16)\n",
    "\n",
    "    # Create checkpoint callback\n",
    "    from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"model_checkpoints\",\n",
    "        filename=f\"{mlflow_run.info.run_id}_last_epoch\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"train_loss\",\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer = lightning.Trainer(\n",
    "        accelerator=\"cpu\",\n",
    "        limit_train_batches=3,\n",
    "        max_epochs=2,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "    trainer.fit(model=autoencoder, train_dataloaders=train_loader)\n",
    "\n",
    "    # Get run information\n",
    "    run_id = mlflow_run.info.run_id\n",
    "    ln.context.run.reference = run_id\n",
    "\n",
    "    # save model summary artifact\n",
    "    local_model_summary_path = (\n",
    "        f\"{mlflow_run.info.artifact_uri.removeprefix('file://')}/model_summary.txt\"\n",
    "    )\n",
    "\n",
    "    mlflow_model_summary_af = ln.Artifact(\n",
    "        local_model_summary_path,\n",
    "        key=f\"testmodels/mlflow/{local_model_summary_path}\",\n",
    "        kind=\"model\",\n",
    "    ).save()\n",
    "\n",
    "    # save checkpoint as a model\n",
    "    mlflow_model_ckpt_af = ln.Artifact(\n",
    "        f\"model_checkpoints/{run_id}_last_epoch.ckpt\",\n",
    "        key=\"testmodels/mlflow/litautoencoder.ckpt\",\n",
    "        kind=\"model\",\n",
    "    ).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62a96f",
   "metadata": {},
   "source": [
    "**See the training progress in the `mlflow` UI:**\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/C0seowxsq4Du2B4T0000.png\" alt=\"MLFlow training UI\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3367d8b0",
   "metadata": {},
   "source": [
    "**See the checkpoints:**\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/n0xxFoMRtZPiQ7VT0001.png\" alt=\"MLFlow checkpoints UI\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331ea1e",
   "metadata": {},
   "source": [
    "If later on, you want to re-use the checkpoint, you can get it via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e92101",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.Artifact.get(key=\"testmodels/mlflow/litautoencoder.ckpt\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4a5e1",
   "metadata": {},
   "source": [
    "Or on the CLI:\n",
    "```\n",
    "lamin get artifact --key 'testmodels/litautoencoder'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becc480",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc136bb",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!rm -rf ./lamin-mlops\n",
    "!lamin delete --force lamin-mlops"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamindb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
