{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "[![](https://img.shields.io/badge/Source%20on%20GitHub-orange)](https://github.com/laminlabs/lamin-mlops/blob/main/docs/wandb.ipynb)\n",
    "[![](https://img.shields.io/badge/Source%20%26%20report%20on%20LaminHub-mediumseagreen)](https://lamin.ai/laminlabs/lamindata/transform/nrPNwWEVUsL95zKv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Weights & Biases\n",
    "\n",
    "We show how LaminDB can be integrated with W&B to track the training process and associate datasets & parameters with models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# pip install lamindb torchvision lightning wandb\n",
    "!lamin init --storage ./lamin-mlops\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import lamindb as ln\n",
    "import wandb\n",
    "import lightning as pl\n",
    "from pathlib import Path\n",
    "\n",
    "from torch import utils\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from autoencoder import LitAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\"hidden_size\": 32, \"bottleneck_size\": 16, \"batch_size\": 32}\n",
    "\n",
    "hyperparameter = ln.Feature(name=\"Autoencoder hyperparameter\", is_type=True).save()\n",
    "hyperparams = ln.Feature.from_dict(MODEL_CONFIG, feature_type=hyperparameter)\n",
    "ln.save(hyperparams)\n",
    "\n",
    "metrics_to_annotate = [\"train_loss\", \"val_loss\", \"current_epoch\"]\n",
    "for metric in metrics_to_annotate:\n",
    "    dtype = int if metric == \"current_epoch\" else float\n",
    "    ln.Feature(name=metric, dtype=dtype).save()\n",
    "\n",
    "# create all Wandb related features like 'wandb_run_id'\n",
    "_ = ln.examples.ml_tracking.create_wandb_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track this notebook/script run so that all checkpoint artifacts are associated with the source code\n",
    "ln.track(params=MODEL_CONFIG, project=ln.Project(name=\"Wandb tutorial\").save())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Define a model\n",
    "\n",
    "We use a basic PyTorch Lightning autoencoder as an example model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "````{dropdown} Code of LitAutoEncoder\n",
    "```{eval-rst}\n",
    ".. literalinclude:: autoencoder.py\n",
    "   :language: python\n",
    "   :caption: Simple autoencoder model\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Query & download the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We saved the MNIST dataset in [curation notebook](/mnist) which now shows up in the Artifact registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Artifact.filter(kind=\"dataset\").to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "You can also find it on lamin.ai if you were connected your instance.\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/LlMSvBjHuXbs36TBGoCM.png\" alt=\"instance view\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Let's get the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "artifact = ln.Artifact.get(key=\"testdata/mnist\")\n",
    "artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "And download it to a local cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "path = artifact.cache()\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Create a PyTorch-compatible dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = MNIST(path.as_posix(), transform=ToTensor())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Monitor training with wandb\n",
    "\n",
    "Train our example model and track the training progress with `wandb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "# create the data loader\n",
    "train_dataset = MNIST(root=\"./data\", train=True, download=True, transform=ToTensor())\n",
    "val_dataset = MNIST(root=\"./data\", train=False, download=True, transform=ToTensor())\n",
    "train_loader = utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "val_loader = utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# init model\n",
    "autoencoder = LitAutoEncoder(\n",
    "    MODEL_CONFIG[\"hidden_size\"], MODEL_CONFIG[\"bottleneck_size\"]\n",
    ")\n",
    "\n",
    "# initialize the logger\n",
    "wandb_logger = WandbLogger(project=\"lamin\")\n",
    "\n",
    "# add batch size to the wandb config\n",
    "wandb_logger.experiment.config[\"batch_size\"] = MODEL_CONFIG[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a LaminDB LightningCallback which also (optionally) annotates checkpoints by desired metrics\n",
    "wandb_logger.experiment.id\n",
    "lamindb_callback = ln.integrations.lightning.Callback(\n",
    "    path=Path(\"model_checkpoints\") / \"{wanddblogger.version}_last_epoch.ckpt\",\n",
    "    key=f\"testmodels/wandb/{wandb_logger.experiment.id}.ckpt\",\n",
    "    features={\n",
    "        \"wandb_run_id\": wandb_logger.experiment.id,\n",
    "        \"wandb_run_name\": wandb_logger.experiment.name,\n",
    "        **{\n",
    "            metric: None for metric in metrics_to_annotate\n",
    "        },  # auto-populated through callback\n",
    "    },\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer = pl.Trainer(\n",
    "    limit_train_batches=3,\n",
    "    max_epochs=5,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[lamindb_callback],\n",
    ")\n",
    "trainer.fit(\n",
    "    model=autoencoder, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "wandb_logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "wandb_logger.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "**See the training progress in the `wandb` UI:**\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/awrTvbxrLaiNav17VxBN.png\" alt=\"Wandb training ui\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Save model in LaminDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "**See the checkpoints:**\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/248fOMXqxT0U4f7LRSgj.png\" alt=\"Wandb check points\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "All checkpoints are automatically annotated by the specified training metrics and MLflow run ID & name to keep both frameworks in sync:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint_af = ln.Artifact.filter(\n",
    "    key__startswith=\"testmodels/wandb/\", suffix__endswith=\"ckpt\", is_latest=True\n",
    ").last()\n",
    "last_checkpoint_af.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "If later on, you want to re-use the checkpoint, you can download it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "last_checkpoint_af.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamindb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
