{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ff2cca",
   "metadata": {},
   "source": [
    "[![](https://img.shields.io/badge/Source%20on%20GitHub-orange)](https://github.com/laminlabs/lamin-mlops/blob/main/docs/wandb.ipynb)\n",
    "[![](https://img.shields.io/badge/Source%20%26%20report%20on%20LaminHub-mediumseagreen)](https://lamin.ai/laminlabs/lamindata/transform/nrPNwWEVUsL95zKv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bff135177a7ae90",
   "metadata": {},
   "source": [
    "# Wandb\n",
    "\n",
    "We show how LaminDB can be integrated with Wandb to track the whole training process, associate data with models, and facilitate model querying based on hyperparameters, among other criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1428ec774d35",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# !pip install -q 'lamindb[jupyter,aws]' torch torchvision lightning wandb\n",
    "!lamin init --storage ./lamin-mlops\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efa3d458f7d713",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import lamindb as ln\n",
    "import wandb\n",
    "\n",
    "ln.context.uid = \"tULn4Va2yERp0000\"\n",
    "ln.context.track()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271521b-0791-4df3-b043-3acc13b3f54c",
   "metadata": {},
   "source": [
    "## Define a model\n",
    "\n",
    "Define a simple autoencoder as an example model using PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83955de5-52f7-41b2-bf42-ab4b0cca5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn, utils\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning\n",
    "\n",
    "\n",
    "class LitAutoEncoder(lightning.LightningModule):\n",
    "    def __init__(self, hidden_size, bottleneck_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, hidden_size), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_size, bottleneck_size)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_size, hidden_size), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_size, 28 * 28)\n",
    "        )\n",
    "        # save hyper-parameters to self.hparams auto-logged by wandb\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3f68c-6e10-4b95-a4f5-729704690a25",
   "metadata": {},
   "source": [
    "## Query & download the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03ee9c-eadd-479f-beca-ad84e4118a6e",
   "metadata": {},
   "source": [
    "We curated the MNIST dataset in [this notebook](https://lamin.ai/laminlabs/lamindata/transform/mwaEQepEtFeh5zKv) and it now shows up in the artifact registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Artifact.filter(type=\"dataset\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19d468",
   "metadata": {},
   "source": [
    "You can also see it on lamin.ai if you connected your instance.\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/LlMSvBjHuXbs36TBGoCM.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea687f6a",
   "metadata": {},
   "source": [
    "Let's get the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0bbe9-3429-431b-addf-8f75652c8df9",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "artifact = ln.Artifact.get(key=\"testdata/mnist\")\n",
    "artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524876dd",
   "metadata": {},
   "source": [
    "And download it to a local cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485a6c3",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "path = artifact.cache()\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb6c06",
   "metadata": {},
   "source": [
    "Create a pytorch-compatible dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c390c0",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = MNIST(path.as_posix(), transform=ToTensor())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dd3b1-a200-467b-9f3d-3c19dae84495",
   "metadata": {},
   "source": [
    "## Monitor training with wandb\n",
    "\n",
    "Train our example model and track the training progress with `wandb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04e4ae-0c89-46e1-9e55-593bcded67e2",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"hidden_size\": 32,\n",
    "    \"bottleneck_size\": 16,\n",
    "    \"batch_size\": 32\n",
    "}\n",
    "\n",
    "# create the data loader\n",
    "train_loader = utils.data.DataLoader(dataset, batch_size=MODEL_CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# init model\n",
    "autoencoder = LitAutoEncoder(MODEL_CONFIG[\"hidden_size\"], MODEL_CONFIG[\"bottleneck_size\"])\n",
    "\n",
    "# initialize the logger\n",
    "wandb_logger = WandbLogger(project=\"lamin\")\n",
    "\n",
    "# add batch size to the wandb config\n",
    "wandb_logger.experiment.config[\"batch_size\"] = MODEL_CONFIG[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d1dee-0e04-4150-898e-deb4e6060f31",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "# store checkpoints to disk and upload to LaminDB after training\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"model_checkpoints/{wandb_logger.version}\", \n",
    "    filename=\"last_epoch\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"train_loss\"\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer = lightning.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    limit_train_batches=3, \n",
    "    max_epochs=2,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98ca68-6a6e-4c9d-bbfa-9d31eae2eb76",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "wandb_logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583004ac-aead-4ca5-8043-d5271af934e7",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "wandb_logger.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b4b6da4ada952",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cddbb5-50a6-4ef9-8534-f4bad6c07af6",
   "metadata": {},
   "source": [
    "**See the training progress in the `wandb` UI:**\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/awrTvbxrLaiNav17VxBN.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62012e5c-e6bb-4b6e-a0cb-0becf9e495ea",
   "metadata": {},
   "source": [
    "## Save model in LaminDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a85a7-f7e8-4c81-bbe1-050659016d98",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# save checkpoint as a model in LaminDB\n",
    "artifact = ln.Artifact(\n",
    "    f\"model_checkpoints/{wandb_logger.version}\",\n",
    "    key=\"testmodels/litautoencoder\",  # is automatically versioned\n",
    "    type=\"model\",\n",
    ").save()\n",
    "\n",
    "# create a label with the wandb experiment name\n",
    "experiment_label = ln.ULabel(\n",
    "    name=wandb_logger.experiment.name, \n",
    "    description=\"wandb experiment name\"\n",
    ").save()\n",
    "\n",
    "# annotate the model artifact\n",
    "artifact.ulabels.add(experiment_label)\n",
    "\n",
    "# define the associated model hyperparameters in ln.Param\n",
    "for k, v in MODEL_CONFIG.items():\n",
    "    ln.Param(name=k, dtype=type(v).__name__).save()\n",
    "artifact.params.add_values(MODEL_CONFIG)\n",
    "\n",
    "# describe the artifact\n",
    "artifact.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2614158-cc36-4e52-87f4-32151e93b6da",
   "metadata": {},
   "source": [
    "**See the checkpoints:**\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/248fOMXqxT0U4f7LRSgj.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51300c29",
   "metadata": {},
   "source": [
    "If later on, you want to re-use the checkpoint, you can download it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf09a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Artifac.get(key='testmodels/litautoencoder').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83bf6c",
   "metadata": {},
   "source": [
    "Or on the CLI:\n",
    "```\n",
    "lamin get artifact --key 'testmodels/litautoencoder'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0705e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save notebook\n",
    "# ln.context.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
