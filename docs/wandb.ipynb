{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bff135177a7ae90",
   "metadata": {},
   "source": [
    "# Wandb\n",
    "\n",
    "We show how LaminDB can be integrated with Wandb to track the whole training process, associate data with models, and facilitate model querying based on hyperparameters, among other criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4df14aefff576a",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# uncomment below to install necessary dependencies for this notebook:\n",
    "# !pip install 'lamindb[jupyter,aws]' -q\n",
    "# !pip install wandb -qU\n",
    "# !pip install torch torchvision torchaudio lightning -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1428ec774d35",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# you can also pass s3://my-bucket\n",
    "!lamin init --storage ./lamin-mlops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efa3d458f7d713",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import lamindb as ln\n",
    "import wandb\n",
    "\n",
    "ln.settings.transform.stem_uid = \"tULn4Va2yERp\"\n",
    "ln.settings.transform.version = \"1\"\n",
    "\n",
    "ln.track()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b9327e978259",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271521b-0791-4df3-b043-3acc13b3f54c",
   "metadata": {},
   "source": [
    "## Define a model\n",
    "\n",
    "Define a simple autoencoder as an example model using PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83955de5-52f7-41b2-bf42-ab4b0cca5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn, utils\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "class LitAutoEncoder(L.LightningModule):\n",
    "    def __init__(self, hidden_size, bottleneck_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, hidden_size), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_size, bottleneck_size)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_size, hidden_size), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_size, 28 * 28)\n",
    "        )\n",
    "        # save hyper-parameters to self.hparams auto-logged by wandb\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3f68c-6e10-4b95-a4f5-729704690a25",
   "metadata": {},
   "source": [
    "## Query & cache MNIST dataset from LaminDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03ee9c-eadd-479f-beca-ad84e4118a6e",
   "metadata": {},
   "source": [
    "**The dataset shows up in LaminHub:**\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/LlMSvBjHuXbs36TBGoCM.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea687f6a",
   "metadata": {},
   "source": [
    "We can either query it by UID from there or query it by any other metadata combination.\n",
    "\n",
    "Here, by description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0bbe9-3429-431b-addf-8f75652c8df9",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "training_data_artifact = ln.Artifact.filter(description=\"MNIST-dataset\").one()\n",
    "training_data_artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524876dd",
   "metadata": {},
   "source": [
    "Let's cache the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = training_data_artifact.cache()\n",
    "cache_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb6c06",
   "metadata": {},
   "source": [
    "Create a pytorch-compatible dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -r /Users/falexwolf/repos/lamin-mlops/docs/lamin-mlops/.lamindb/zAYD0B1Rw7lBFsJS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c390c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(cache_path / \"raw\", transform=ToTensor())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dd3b1-a200-467b-9f3d-3c19dae84495",
   "metadata": {},
   "source": [
    "## Monitor training with wandb\n",
    "\n",
    "Train our example model and track training progress with Wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2595336-fc58-4203-b859-28fbb49bd344",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    \"hidden_size\": 32,\n",
    "    \"bottleneck_size\": 16,\n",
    "    \"batch_size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb33a95df37e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PyTorch dataloader\n",
    "train_loader = utils.data.DataLoader(dataset, batch_size=MODEL_CONFIG[\"batch_size\"], shuffle=True)\n",
    "# init model\n",
    "autoencoder = LitAutoEncoder(MODEL_CONFIG[\"hidden_size\"], MODEL_CONFIG[\"bottleneck_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04e4ae-0c89-46e1-9e55-593bcded67e2",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "# initialise the wandb logger\n",
    "wandb_logger = WandbLogger(project=\"lamin\")\n",
    "# add batch size to the wandb config\n",
    "wandb_logger.experiment.config[\"batch_size\"] = MODEL_CONFIG[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d1dee-0e04-4150-898e-deb4e6060f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "# store checkpoints to disk and upload to LaminDB after training\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"model_checkpoints/{wandb_logger.version}\", \n",
    "    filename=\"last_epoch\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"train_loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a9295-90a4-4dcd-8054-19ac2eb44e49",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    limit_train_batches=3, \n",
    "    max_epochs=2,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98ca68-6a6e-4c9d-bbfa-9d31eae2eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583004ac-aead-4ca5-8043-d5271af934e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b4b6da4ada952",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cddbb5-50a6-4ef9-8534-f4bad6c07af6",
   "metadata": {},
   "source": [
    "**Check out the training progress on the Wandb UI:**\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/awrTvbxrLaiNav17VxBN.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62012e5c-e6bb-4b6e-a0cb-0becf9e495ea",
   "metadata": {},
   "source": [
    "## Save model in LaminDB\n",
    "\n",
    "Upload the model checkpoint of the trained model to LaminDB.\n",
    "\n",
    "We annotate the LaminDB Artifact with the wandb experiment ID and the hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a85a7-f7e8-4c81-bbe1-050659016d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint in LaminDB\n",
    "ckpt_artifact = ln.Artifact(\n",
    "    f\"model_checkpoints/{wandb_logger.version}\",\n",
    "    description=\"model-checkpoint\",\n",
    "    type=\"model\",\n",
    ").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43efe72-8831-4caa-afca-4941d511ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a label with the wandb experiment name\n",
    "experiment_label = ln.ULabel(\n",
    "    name=wandb_logger.experiment.name, \n",
    "    description=\"wandb experiment name\"\n",
    ").save()\n",
    "# annotate the artifact\n",
    "ckpt_artifact.ulabels.add(experiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb379b6-eac2-4cbc-aa56-b839e413a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the associated model hyperparameters in ln.Param\n",
    "for k, v in MODEL_CONFIG.items():\n",
    "    ln.Param(name=k, dtype=type(v).__name__).save()\n",
    "# annotate the artifact with them\n",
    "ckpt_artifact.params.add_values(MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe39ae23-57c5-4aaa-86f5-60f1d9fc6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show info about the checkpoint artifact\n",
    "ckpt_artifact.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2614158-cc36-4e52-87f4-32151e93b6da",
   "metadata": {},
   "source": [
    "**Look at saved checkpoints in LaminHub:**\n",
    "\n",
    "<img src=\"https://lamin-site-assets.s3.amazonaws.com/.lamindb/248fOMXqxT0U4f7LRSgj.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0705e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save notebook\n",
    "# ln.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
