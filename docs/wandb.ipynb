{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "[![](https://img.shields.io/badge/Source%20on%20GitHub-orange)](https://github.com/laminlabs/lamin-mlops/blob/main/docs/wandb.ipynb)\n",
    "[![](https://img.shields.io/badge/Source%20%26%20report%20on%20LaminHub-mediumseagreen)](https://lamin.ai/laminlabs/lamindata/transform/nrPNwWEVUsL95zKv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Weights & Biases\n",
    "\n",
    "LaminDB can be integrated with W&B to track the training process and associate datasets & parameters with models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# pip install lamindb torchvision lightning wandb\n",
    "!lamin init --storage ./lamin-mlops\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "import lamindb as ln\n",
    "from lamindb.integrations import lightning as ll\n",
    "import wandb\n",
    "\n",
    "from torch import utils\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from autoencoder import LitAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# define model run parameters, features, and labels so that validation passes later on\n",
    "MODEL_CONFIG = {\"hidden_size\": 32, \"bottleneck_size\": 16, \"batch_size\": 32}\n",
    "\n",
    "hyperparameter = ln.Feature(name=\"Autoencoder hyperparameter\", is_type=True).save()\n",
    "hyperparams = ln.Feature.from_dict(MODEL_CONFIG, type=hyperparameter)\n",
    "ln.save(hyperparams)\n",
    "\n",
    "metrics_to_annotate = [\"train_loss\", \"val_loss\", \"current_epoch\"]\n",
    "for metric in metrics_to_annotate:\n",
    "    dtype = int if metric == \"current_epoch\" else float\n",
    "    ln.Feature(name=metric, dtype=dtype).save()\n",
    "\n",
    "# create all Wandb related features like 'wandb_run_id'\n",
    "ln.examples.wandb.save_wandb_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# track this notebook/script run so that all checkpoint artifacts are associated with the source code\n",
    "ln.track(params=MODEL_CONFIG, project=ln.Project(name=\"Wandb tutorial\").save())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Define a model\n",
    "\n",
    "We use a basic PyTorch Lightning autoencoder as an example model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "````{dropdown} Code of LitAutoEncoder\n",
    "```{eval-rst}\n",
    ".. literalinclude:: autoencoder.py\n",
    "   :language: python\n",
    "   :caption: Simple autoencoder model\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Query & download the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We saved the MNIST dataset in a [curation notebook](/mnist) which now shows up in the Artifact registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.Artifact.filter(kind=\"dataset\").to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Let's get the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "mnist_af = ln.Artifact.get(key=\"testdata/mnist\")\n",
    "mnist_af"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "And download it to a local cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "path = mnist_af.cache()\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Create a PyTorch-compatible dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "mnist_dataset = MNIST(path.as_posix(), transform=ToTensor())\n",
    "mnist_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Monitor training with wandb\n",
    "\n",
    "Train our example model and track the training progress with `wandb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "# create the data loader\n",
    "train_dataset = MNIST(root=\"./data\", train=True, download=True, transform=ToTensor())\n",
    "val_dataset = MNIST(root=\"./data\", train=False, download=True, transform=ToTensor())\n",
    "train_loader = utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "val_loader = utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# init model\n",
    "autoencoder = LitAutoEncoder(\n",
    "    MODEL_CONFIG[\"hidden_size\"], MODEL_CONFIG[\"bottleneck_size\"]\n",
    ")\n",
    "\n",
    "# initialize the logger\n",
    "wandb_logger = WandbLogger(project=\"lamin\")\n",
    "\n",
    "# add batch size to the wandb config\n",
    "wandb_logger.experiment.config[\"batch_size\"] = MODEL_CONFIG[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a LaminDB LightningCallback which also (optionally) annotates checkpoints by desired metrics\n",
    "wandb_logger.experiment.id\n",
    "lamindb_callback = ll.Checkpoint(\n",
    "    dirpath=f\"testmodels/wandb/{wandb_logger.experiment.id}.ckpt\",\n",
    "    features={\n",
    "        \"wandb_run_id\": wandb_logger.experiment.id,\n",
    "        \"wandb_run_name\": wandb_logger.experiment.name,\n",
    "        **{\n",
    "            metric: None for metric in metrics_to_annotate\n",
    "        },  # auto-populated through callback\n",
    "    },\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer = pl.Trainer(\n",
    "    limit_train_batches=3,\n",
    "    max_epochs=5,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[lamindb_callback],\n",
    ")\n",
    "trainer.fit(\n",
    "    model=autoencoder, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "wandb_logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## W&B and LaminDB user interfaces together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "**W&B and LaminDB runs:**\n",
    "\n",
    "Both W&B and LaminDB capture any runs together with run parameters.\n",
    "\n",
    "| W&B experiment overview | LaminHub run overview |\n",
    "| ------- | ------- |\n",
    "| [![W&B experiment overview UI](https://lamin-site-assets.s3.amazonaws.com/.lamindb/DkMfwknGEBZ0EdTf0000.png)](https://lamin-site-assets.s3.amazonaws.com/.lamindb/DkMfwknGEBZ0EdTf0000.png) | [![LaminHub run UI](https://lamin-site-assets.s3.amazonaws.com/.lamindb/wpfQM12SXxY7owqR0000.png)](https://lamin-site-assets.s3.amazonaws.com/.lamindb/wpfQM12SXxY7owqR0000.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "**W&B run details and LaminDB artifact details:**\n",
    "\n",
    "W&B and LaminDB complement each other.\n",
    "Whereas W&B is excellent at capturing metrics over time, LaminDB excells at capturing lineage of input & output data and training checkpoints.\n",
    "\n",
    "| W&B run view | LaminHub run view |\n",
    "| ------------- | ------------------ |\n",
    "| [![W&B runs](https://lamin-site-assets.s3.amazonaws.com/.lamindb/Qunk9d9YvAcEhGjI0000.png)](https://lamin-site-assets.s3.amazonaws.com/.lamindb/Qunk9d9YvAcEhGjI0000.png) | [![Laminhub run lineage](https://lamin-site-assets.s3.amazonaws.com/.lamindb/oc4qSs8xvDjSw5g90000.png)](https://lamin-site-assets.s3.amazonaws.com/.lamindb/oc4qSs8xvDjSw5g90000.png) |\n",
    "\n",
    "Both frameworks display output artifacts that were generated during the run.\n",
    "LaminDB further captures input artifacts, their origin and the associated source code.\n",
    "\n",
    "| W&B artifact view | LaminHub artifact view |\n",
    "| ------------- | --------------------- |\n",
    "| [![W&B artifact UI](https://lamin-site-assets.s3.amazonaws.com/.lamindb/I20YlrvMhvvqIwqG0000.png)](https://lamin-site-assets.s3.amazonaws.com/.lamindb/I20YlrvMhvvqIwqG0000.png) | [![LaminHub artifact UI](https://lamin-site-assets.s3.amazonaws.com/.lamindb/rFHt5FeXgWp9nrtz0000.png)](https://lamin-site-assets.s3.amazonaws.com/.lamindb/rFHt5FeXgWp9nrtz0000.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "All checkpoints are automatically annotated by the specified training metrics and W&B run ID & name to keep both frameworks in sync:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "last_checkpoint_af = (\n",
    "    ln.Artifact.filter(is_best_model=True)\n",
    "    .filter(suffix__endswith=\"ckpt\", is_latest=True)\n",
    "    .last()\n",
    ")\n",
    "last_checkpoint_af.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "To reuse the checkpoint later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "last_checkpoint_af.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "last_checkpoint_af.view_lineage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Features associated with a whole training run are annotated on a run level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.context.run.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
