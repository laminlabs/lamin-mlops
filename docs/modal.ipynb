{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5781ae-6493-46af-bea1-727fee1a1e42",
   "metadata": {},
   "source": [
    "# Lamin Compute with Modal Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34e2e5-e596-4c26-8771-e261b96fd4c7",
   "metadata": {},
   "source": [
    "We show how to run compute jobs with Modal backend to run any script in the cloud without having to manage compute resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6fb3c2-a63b-4e79-b458-b0d24a3a444b",
   "metadata": {},
   "source": [
    "There are three major components to run compute jobs seemlessly in the cloud.\n",
    "\n",
    "1. Access to your Artifacts and Data - (Lamin) \n",
    "2. Seemless access to a compute backend to run any workload - (Modal)\n",
    "3. Tracking your workload inputs and outputs - (Lamin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3408b05-34de-46a9-98ea-28be33b38809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KThe web browser should have opened for you to authenticate and get an API token.\n",
      "If it didn't, please copy this URL into your web browser manually:\n",
      "\n",
      "\u001b[2K\u001b]8;id=311415;https://modal.com/token-flow/tf-3hZMeO36QqLSRCpsT0RADS\u001b\\\u001b[4;94mhttps://modal.com/token-flow/tf-3hZMeO36QqLSRCpsT0RADS\u001b[0m\u001b]8;;\u001b\\\n",
      "\n",
      "\u001b[2K\u001b[32m⠙\u001b[0m Waiting for authentication in the web browser\n",
      "\u001b[2K\u001b[32m⠇\u001b[0m Waiting for token flow to complete...omplete...\n",
      "\u001b[1A\u001b[2K\u001b[32mWeb authentication finished successfully!\u001b[0m\n",
      "\u001b[32mToken is connected to the \u001b[0m\u001b[35mragyhaddad\u001b[0m\u001b[32m workspace.\u001b[0m\n",
      "Verifying token against \u001b[4;34mhttps://api.modal.com\u001b[0m\n",
      "\u001b[32mToken verified successfully!\u001b[0m\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Storing token\n",
      "\u001b[1A\u001b[2K\u001b[32mToken written to \u001b[0m\u001b[35m/Users/rhaddad/\u001b[0m\u001b[35m.modal.toml\u001b[0m\u001b[32m in profile \u001b[0m\u001b[35mragyhaddad\u001b[0m\u001b[32m.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install modal 'lamindb[jupyter]'\n",
    "!modal setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c754b-70e7-4467-bfe8-ed3af99171e1",
   "metadata": {},
   "source": [
    "## Define your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682a2ee-05e1-40a1-ab99-e86741abe48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lamindb as ln\n",
    "\n",
    "API_KEY = os.environ['lamin_user_api_key']\n",
    "PROJECT_NAME = os.environ['lamin_project_name']\n",
    "\n",
    "# LAMIN SETUP\n",
    "ln.setup.login(api_key=API_KEY)\n",
    "ln.connect('laminlabs/lamindata')\n",
    "my_project = ln.Project(name=PROJECT_NAME).save()\n",
    "\n",
    "\n",
    "ln.track(project=PROJECT_NAME)\n",
    "\n",
    "## Code Start \n",
    "def say_hello():\n",
    "    print('Hello, World! lamin, user key has been passed successfully')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    say_hello()\n",
    "\n",
    "## Code End\n",
    "ln.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d86cd-fa5b-4a5f-90ad-526ed27320f4",
   "metadata": {},
   "source": [
    "we save the above script as `./helloworld.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9cf0f0-1b1e-4b13-adab-a812e7806373",
   "metadata": {},
   "source": [
    "Execute the script in the cloud by running:\n",
    "```bash \n",
    "lamin run ./helloworld.py --project modal_project\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc1499-c7eb-4866-9b07-621c6b637014",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lamin run ./helloworld.py --project modal_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec68fc-219e-4049-93fe-a53da3ff806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lamindb as ln\n",
    "\n",
    "API_KEY = os.environ['lamin_user_api_key']\n",
    "PROJECT_NAME = os.environ['lamin_project_name']\n",
    "\n",
    "# LAMIN SETUP\n",
    "ln.setup.login(api_key=API_KEY)\n",
    "ln.connect('laminlabs/lamindata')\n",
    "ln.track()  # track your run of a notebook or script \n",
    "\n",
    "def main():\n",
    "    # Access inputs -------------------------------------------\n",
    "    artifact = ln.Artifact.using(\"laminlabs/cellxgene\").get(\"7dVluLROpalzEh8m\")  # query the artifact https://lamin.ai/laminlabs/cellxgene/artifact/7dVluLROpalzEh8m\n",
    "    adata = artifact.load()[:, :100]  # load into memory or sync to cache: filepath = artifact.cache()\n",
    "    \n",
    "    # Your transformation -------------------------------------\n",
    "    \n",
    "    import scanpy as sc  # find marker genes with Scanpy\n",
    "    \n",
    "    sc.pp.normalize_total(adata)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.tl.rank_genes_groups(adata, groupby=\"cell_type\")\n",
    "    \n",
    "    # Save outputs --------------------------------------------\n",
    "    \n",
    "    ln.Artifact.from_anndata(adata, key=\"my-datasets/my-result.h5ad\").save()  # save versioned output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "ln.finish()  # finish the run, save source code & run report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf3076-76f7-4710-8525-6ebc026be074",
   "metadata": {},
   "source": [
    "Save your script locally `./lamin_sc.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe7517-f145-4fae-80e1-6f4d6334dc6e",
   "metadata": {},
   "source": [
    "Then execute your script using \n",
    "```\n",
    "lamin run lamin_sc.py --project lamin_sc --packages scanpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c823b7ce-7698-4a35-a485-82ed826a4db0",
   "metadata": {},
   "source": [
    "## Specifying dependancies and images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a46ec-24b7-4a52-b8a4-b903fdbf225a",
   "metadata": {},
   "source": [
    "A key component to run compute workloads is access to defining images and environments required to run your code. Below we go over examples where we include custom dependancies and images to the environment to run your code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae46c799-63cc-4ce9-aa15-31bd24b7a724",
   "metadata": {},
   "source": [
    "### Save your local script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a20ea-75e1-455b-914a-6d9c33bb56b2",
   "metadata": {},
   "source": [
    "Below we write a pytorch script and add GPU access to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7860be0f-7522-4721-b0eb-873cd53383c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lamindb as ln\n",
    "\n",
    "API_KEY = os.environ['lamin_user_api_key']\n",
    "PROJECT_NAME = os.environ['lamin_project_name']\n",
    "\n",
    "# LAMIN SETUP\n",
    "ln.setup.login(api_key=API_KEY)\n",
    "ln.connect('laminlabs/lamindata')\n",
    "my_project = ln.Project(name=PROJECT_NAME).save()\n",
    "\n",
    "ln.track(project=PROJECT_NAME)\n",
    "\n",
    "def say_hello():\n",
    "    import torch\n",
    "    print('Imported pytorch and detecting GPUs')\n",
    "    print(torch.cuda.is_available())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    say_hello()\n",
    "\n",
    "ln.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a62ee-fdfa-44bf-a3a2-147dbe9ac1fd",
   "metadata": {},
   "source": [
    "We save the above script locally as `./helloworld_gpu.py` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77327365-4a23-4b86-b1de-896e524b4f4f",
   "metadata": {},
   "source": [
    "To execute your code in the cloud run the following command\n",
    "``` bash \n",
    "lamin run ./helloworld_gpu.py --project lamin_project_gpu --image nvcr.io/nvidia/pytorch:22.12-py3 --packages torch,numpy --gpu T4:1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603e514-0a90-45d2-93cb-e0fc842b9389",
   "metadata": {},
   "source": [
    "Notice in the above command we specify the `--image` and we additionally add GPU access by passing `--gpu` flag where we specify `T4:1` as the GPU type and number of GPUs to attach to our compute job.\n",
    "\n",
    "We can also specify additional `pip` dependancies via `--packages` `torch,numpy,seaborn`  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
